{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertTokenizer,TFBertModel,TFBertForSequenceClassification\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data to train the model for restaurent recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMMAD\\AppData\\Local\\Temp\\ipykernel_13592\\402519286.py:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/restaurants_data_analysis.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 250329 entries, 8 to 267377\n",
      "Data columns (total 23 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   budget                      250329 non-null  int64  \n",
      " 1   is_new_until                248498 non-null  object \n",
      " 2   latitude                    250329 non-null  float64\n",
      " 3   longitude                   250329 non-null  float64\n",
      " 4   minimum_delivery_time       250329 non-null  int64  \n",
      " 5   minimum_order_amount        250329 non-null  int64  \n",
      " 6   minimum_pickup_time         250329 non-null  int64  \n",
      " 7   name                        250329 non-null  object \n",
      " 8   post_code                   250317 non-null  object \n",
      " 9   rating                      250329 non-null  float64\n",
      " 10  review_number               250329 non-null  int64  \n",
      " 11  review_with_comment_number  250329 non-null  int64  \n",
      " 12  vertical                    250328 non-null  object \n",
      " 13  vertical_parent             250329 non-null  object \n",
      " 14  delivery_provider           250329 non-null  object \n",
      " 15  is_active                   250329 non-null  bool   \n",
      " 16  is_new                      250329 non-null  bool   \n",
      " 17  is_promoted                 250329 non-null  bool   \n",
      " 18  city                        250329 non-null  object \n",
      " 19  timezone                    250329 non-null  object \n",
      " 20  dine_in                     250329 non-null  bool   \n",
      " 21  main_cuisine                250329 non-null  object \n",
      " 22  country                     250329 non-null  object \n",
      "dtypes: bool(4), float64(3), int64(6), object(10)\n",
      "memory usage: 39.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/restaurants_data_analysis.csv')\n",
    "df = df.dropna(subset=['latitude', 'longitude', 'main_cuisine', 'budget'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making and training the model for restaurent recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7823/7823 [==============================] - 19s 2ms/step - loss: 0.2484 - accuracy: 0.9474\n",
      "Epoch 2/10\n",
      "7823/7823 [==============================] - 19s 2ms/step - loss: 0.0222 - accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "7823/7823 [==============================] - 19s 2ms/step - loss: 0.0149 - accuracy: 0.9965\n",
      "Epoch 4/10\n",
      "7823/7823 [==============================] - 18s 2ms/step - loss: 0.0119 - accuracy: 0.9973\n",
      "Epoch 5/10\n",
      "7823/7823 [==============================] - 34s 4ms/step - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 6/10\n",
      "7823/7823 [==============================] - 40s 5ms/step - loss: 0.0090 - accuracy: 0.9979\n",
      "Epoch 7/10\n",
      "7823/7823 [==============================] - 42s 5ms/step - loss: 0.0084 - accuracy: 0.9981\n",
      "Epoch 8/10\n",
      "7823/7823 [==============================] - 41s 5ms/step - loss: 0.0081 - accuracy: 0.9982\n",
      "Epoch 9/10\n",
      "7823/7823 [==============================] - 42s 5ms/step - loss: 0.0082 - accuracy: 0.9982\n",
      "Epoch 10/10\n",
      "7823/7823 [==============================] - 28s 4ms/step - loss: 0.0074 - accuracy: 0.9983\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(df['main_cuisine'])\n",
    "with open('labelEncoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Step 2: One-hot encode the target labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
    "\n",
    "# Step 3: Tokenize and vectorize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['main_cuisine'])\n",
    "sequences = tokenizer.texts_to_sequences(df['main_cuisine'])\n",
    "input_data = pad_sequences(sequences)\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_dim = 50\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=input_data.shape[1]),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(y_train_onehot.shape[1], activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data, y_train_onehot, epochs=10, batch_size=32)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('restaurant_recommendation_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if the model is working or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "227/227 [==============================] - 0s 1ms/step\n",
      "['R.F.C Biriyani - https://www.google.com/maps/search/?api=1&query=23.1773611,90.20375', 'Cheezy Bite - https://www.google.com/maps/search/?api=1&query=30.2400717,71.4924828', 'Foodlicious - https://www.google.com/maps/search/?api=1&query=31.4255232,73.0726706', 'Golden Bite - https://www.google.com/maps/search/?api=1&query=31.41584198,73.04084615', \"Sariya's Sip N Bite - Gulberg - https://www.google.com/maps/search/?api=1&query=31.4214801,73.0631505\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('data/restaurants_data_analysis.csv')\n",
    "df = df.dropna(subset=['latitude', 'longitude', 'main_cuisine', 'budget'])\n",
    "\n",
    "model = tf.keras.models.load_model('restaurant_recommendation_model.h5')\n",
    "\n",
    "# Load the label encoder used during training\n",
    "# Replace 'your_label_encoder_path.pkl' with the actual file path of the saved label encoder.\n",
    "import pickle\n",
    "with open('labelEncoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Tokenize and vectorize the data (using the same tokenizer as before)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['main_cuisine'])\n",
    "\n",
    "# Function to get multiple restaurant recommendations\n",
    "def get_recommendations(location, cuisine_preference, budget_constraint, num_recommendations=5):\n",
    "    user_input = cuisine_preference\n",
    "    user_sequence = tokenizer.texts_to_sequences([user_input])\n",
    "    user_data = pad_sequences(user_sequence, maxlen=3)  # Assuming the input sequence length is 22 (same as during training)\n",
    "\n",
    "    # Predict the user's preferences using the loaded model\n",
    "    user_preferences = np.array(model.predict(user_data))\n",
    "\n",
    "    # Get the index of the predicted category with the highest probability\n",
    "    predicted_category_index = tf.argmax(user_preferences, axis=1).numpy()[0]\n",
    "\n",
    "    # Get the predicted category using the label encoder\n",
    "    predicted_category = label_encoder.inverse_transform([predicted_category_index])[0]\n",
    "\n",
    "    # Filter restaurants based on the predicted category\n",
    "    filtered_restaurants = df[df['main_cuisine'] == predicted_category]\n",
    "\n",
    "    filtered_restaurants_data = filtered_restaurants['main_cuisine'] + ' ' + filtered_restaurants['budget'].astype(str)\n",
    "    filtered_sequence = tokenizer.texts_to_sequences(filtered_restaurants_data)\n",
    "    filtered_data = pad_sequences(filtered_sequence, maxlen=3)\n",
    "    filtered_predictions = model.predict(filtered_data)\n",
    "    filtered_predictions = np.array(filtered_predictions)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(user_preferences, filtered_predictions)\n",
    "\n",
    "    # Get the index of restaurants with highest similarity\n",
    "    top_indices = cosine_sim.argsort()[0][-num_recommendations:][::-1]\n",
    "\n",
    "    # Create a list to store the recommended restaurants and their Google Maps links\n",
    "    recommended_restaurants = []\n",
    "\n",
    "    for index in top_indices:\n",
    "        restaurant_name = filtered_restaurants.iloc[index]['name']\n",
    "        latitude = filtered_restaurants.iloc[index]['latitude']\n",
    "        longitude = filtered_restaurants.iloc[index]['longitude']\n",
    "\n",
    "        # Construct the Google Maps link\n",
    "        google_maps_link = f\"https://www.google.com/maps/search/?api=1&query={latitude},{longitude}\"\n",
    "\n",
    "        # Combine the restaurant name and Google Maps link\n",
    "        restaurant_info = f\"{restaurant_name} - {google_maps_link}\"\n",
    "        recommended_restaurants.append(restaurant_info)\n",
    "\n",
    "    return recommended_restaurants\n",
    "\n",
    "# Example Usage:\n",
    "location = \"Lahore\"  # Replace with the user's location\n",
    "cuisine_preference = \"Fast Food\"  # Replace with the user's cuisine preference\n",
    "budget_constraint = 20  # Replace with the user's budget constraint\n",
    "\n",
    "recommended_restaurants = get_recommendations(location, cuisine_preference, budget_constraint)\n",
    "print(recommended_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data for NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities = df['city'].unique()\n",
    "unique_cuisines = df['main_cuisine'].unique()\n",
    "\n",
    "# Phrases\n",
    "city_phrases = [\n",
    "    \"I live in city\",\n",
    "    \"I reside in city\",\n",
    "    \"city is my location\",\n",
    "    \"My location is city\",\n",
    "    \"I'm located in city\",\n",
    "    \"My current city is city\",\n",
    "    \"From city here\",\n",
    "    \"Hailing from city\",\n",
    "    \"I am currently living in city\",\n",
    "    \"city is my current location\",\n",
    "    \"I'm in city\",\n",
    "    \"city is my current city\",\n",
    "    \"Living in city\",\n",
    "    \"city is where I live\",\n",
    "    \"city is where I reside\",\n",
    "    \"In city right now\",\n",
    "    \"Currently in city\",\n",
    "    \"city is my place\",\n",
    "    \"city is my home\",\n",
    "    \"My home city is city\"\n",
    "]\n",
    "\n",
    "cuisine_phrases = [\n",
    "    \"I love [cuisine] food\",\n",
    "    \"I'm in the mood for [cuisine]\",\n",
    "    \"I'm craving [cuisine]\",\n",
    "    \"I would like [cuisine] cuisine\",\n",
    "    \"How about [cuisine]\",\n",
    "    \"I want to try [cuisine]\",\n",
    "    \"Let's go for [cuisine]\",\n",
    "    \"I enjoy [cuisine]\",\n",
    "    \"My favorite is [cuisine]\",\n",
    "    \"I'm interested in [cuisine]\",\n",
    "    \"I'm looking for [cuisine]\",\n",
    "    \"[cuisine] is my favorite\"\n",
    "]\n",
    "\n",
    "money_phrases = [\n",
    "    \"I have [amount] to spend\",\n",
    "    \"My budget is [amount]\",\n",
    "    \"I can afford [amount]\",\n",
    "    \"I'm willing to pay [amount]\",\n",
    "    \"I'm ready to spend [amount]\",\n",
    "    \"I want to spend around [amount]\",\n",
    "    \"I have around [amount] for the meal\",\n",
    "    \"I can spend up to [amount]\",\n",
    "    \"I'm looking for something within [amount]\",\n",
    "    \"I'm comfortable spending [amount]\"\n",
    "]\n",
    "\n",
    "# Generate sentences with city names, cuisine names, and random money values\n",
    "sentences = []\n",
    "entities = []\n",
    "outputs = []\n",
    "for _ in range(10000):\n",
    "    # Randomly choose a category: city, cuisine, or money\n",
    "    category = random.choice(['City', 'Cuisine', 'Money'])\n",
    "\n",
    "    if category == 'City':\n",
    "        city = random.choice(unique_cities)\n",
    "        phrase = random.choice(city_phrases).replace(\"city\", city)\n",
    "        entity = 'Location'\n",
    "        output = city\n",
    "\n",
    "    elif category == 'Cuisine':\n",
    "        cuisine = random.choice(unique_cuisines)\n",
    "        phrase = random.choice(cuisine_phrases).replace(\"[cuisine]\", cuisine)\n",
    "        entity = 'Cuisine'\n",
    "        output = cuisine\n",
    "\n",
    "    else:  # category == 'Money'\n",
    "        amount = random.randint(10, 1000)  # Generate a random integer between 10 and 1000\n",
    "        phrase = random.choice(money_phrases).replace(\"[amount]\", str(amount))\n",
    "        entity = 'Money'\n",
    "        output = str(amount)\n",
    "\n",
    "    sentences.append(phrase)\n",
    "    entities.append(entity)\n",
    "    outputs.append(output)\n",
    "\n",
    "# Create a DataFrame with the generated data\n",
    "data = pd.DataFrame({'Sentence': sentences, 'Entity': entities, 'Output': outputs})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv('restaurant_recommendation_data_combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making, training and saving NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data/restaurant_recommendation_data.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "data['Input'] = data['Sentence'] + ' [SEP] ' + data['Entity']\n",
    "input_texts = data['Input'].values\n",
    "labels, unique_labels = pd.factorize(data['Output'])\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(input_texts)\n",
    "sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "input_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_sequences, tf.keras.utils.to_categorical(labels), test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=X_train.shape[1]),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=14, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('saved_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained this model on google colab and results were:\n",
    "<div style=\"font-family: 'Courier New', Courier, monospace; font-size: 15px; font-weight: bold\" >\n",
    "Epoch 90/100<br>\n",
    "572/572 [==============================] - 4s 7ms/step - loss: 0.0145 - accuracy: 0.9984 - val_loss: 0.5237 - val_accuracy: 0.9705<br>\n",
    "Epoch 91/100<br>\n",
    "572/572 [==============================] - 5s 9ms/step - loss: 0.0101 - accuracy: 0.9986 - val_loss: 0.5124 - val_accuracy: 0.9750<br>\n",
    "Epoch 92/100<br>\n",
    "572/572 [==============================] - 4s 7ms/step - loss: 0.0131 - accuracy: 0.9979 - val_loss: 0.5026 - val_accuracy: 0.9760<br>\n",
    "Epoch 93/100<br>\n",
    "572/572 [==============================] - 4s 6ms/step - loss: 0.0260 - accuracy: 0.9940 - val_loss: 0.5112 - val_accuracy: 0.9720<br>\n",
    "Epoch 94/100<br>\n",
    "572/572 [==============================] - 5s 8ms/step - loss: 0.0139 - accuracy: 0.9979 - val_loss: 0.4917 - val_accuracy: 0.9750<br>\n",
    "Epoch 95/100<br>\n",
    "572/572 [==============================] - 4s 7ms/step - loss: 0.0088 - accuracy: 0.9987 - val_loss: 0.4691 - val_accuracy: 0.9765<br>\n",
    "Epoch 96/100<br>\n",
    "572/572 [==============================] - 4s 6ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.4875 - val_accuracy: 0.9760<br>\n",
    "Epoch 97/100<br>\n",
    "572/572 [==============================] - 4s 6ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.4883 - val_accuracy: 0.9765<br>\n",
    "Epoch 98/100<br>\n",
    "572/572 [==============================] - 5s 9ms/step - loss: 0.0709 - accuracy: 0.9821 - val_loss: 0.5418 - val_accuracy: 0.9620<br>\n",
    "Epoch 99/100<br>\n",
    "572/572 [==============================] - 4s 7ms/step - loss: 0.0225 - accuracy: 0.9967 - val_loss: 0.4609 - val_accuracy: 0.9730<br>\n",
    "Epoch 100/100<br>\n",
    "572/572 [==============================] - 4s 6ms/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 0.4873 - val_accuracy: 0.9725<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_labels, f)\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 232 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C92E2A4680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 232 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C92E2A4680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 442ms/step\n",
      "Lahore\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('saved_model.h5')\n",
    "\n",
    "# Preprocess the input sentence and entity\n",
    "def preprocess_input(sentence, entity):\n",
    "    input_text = sentence + ' [SEP] ' + entity\n",
    "    input_sequences = tokenizer.texts_to_sequences([input_text])\n",
    "    input_sequences = pad_sequences(input_sequences, padding='post', maxlen=X_train.shape[1])\n",
    "    return input_sequences\n",
    "\n",
    "# Function to predict the output category\n",
    "def predict_category(sentence, entity):\n",
    "    input_sequences = preprocess_input(sentence, entity)\n",
    "    predictions = model.predict(input_sequences)\n",
    "    predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "    predicted_class = unique_labels[predicted_class_index]\n",
    "    return predicted_class\n",
    "\n",
    "sentence = \"I live in Lahore\"\n",
    "entity = \"Location\"\n",
    "predicted_category = predict_category(sentence, entity)\n",
    "print(predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(input_text):\n",
    "    # Tokenize the input text\n",
    "    tokens = word_tokenize(input_text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Get Part-of-Speech tags for the remaining words\n",
    "    tagged_tokens = pos_tag(filtered_tokens)\n",
    "\n",
    "    return tagged_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
